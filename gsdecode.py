#!/usr/local/bin/pypy3

import math
import collections
import random
import sys
import re
import string
import argparse
import textwrap
import os
import gzip
import json

def load_saved_model(filename):
    with gzip.open(filename, 'rt') as infile:
        ngram_probabilities = json.load(infile)
    return ngram_probabilities

def load_text(filename):
    return list(iter_text(filename))

def iter_text(filename):
    if os.path.isdir(filename):
        files = [os.path.join(filename, f) for f in os.listdir(filename)]
        files = [f for f in files
                 if f.endswith('.txt') and not os.path.isdir(f)]
    else:
        files = [filename]

    for fn in files:
        with open(fn) as datafile:
            text_raw = datafile.read()
            rex = '[^{}]'.format(string.printable)
            text = re.sub(rex, ' ', text_raw)
            yield text

_space_rex = re.compile('\s+')
def uniform_whitespace(text):
    return _space_rex.sub(' ', text)

def strip_punctuation(text):
    punct = string.punctuation
    punct = punct.maketrans(punct, ' ' * len(punct))
    return text.translate(punct)

def strip_digits(text):
    digits = '0123456789'
    digits = digits.maketrans(digits, ' ' * len(digits))
    return text.translate(digits)

def get_ngram_probabilities(text, n):
    if n < 1:
        raise ValueError("Minimum ngram size is 1")

    all_ngrams = []
    for i in range(1, n + 1):
        ngrams = count_ngrams(text, i)
        ngrams = categorical_log_probabilities(ngrams)

        # Here we calculate the probability of the given ngram as the
        # probabilitiy of the last character appearing given the previous
        # characters. This way, we can substitute lower n-gram stats for
        # missing higher n-gram stats in an internally consistent way.
        # This amounts to the assumption that the missing higher n-grams
        # won't affect the marginal probability for lower n-grams.
        for j, sub_ngrams in enumerate(all_ngrams):
            for ng in ngrams:
                ngrams[ng] -= sub_ngrams[ng[:j + 1]]

        all_ngrams.append(ngrams)

    all_ngrams.reverse()
    return all_ngrams

# This is literally identical to the above function, with one exception:
# Instead of factoring out the leading lower n-grams, it factors out the
# trailing lower n-grams. In other words, where the above tries to guess
# the probability of the last character given the first few, this tries
# to guess the probability of the first character given the last few.
def get_ngram_head_probabilities(text, n):
    if n < 1:
        raise ValueError("Minimum ngram size is 1")

    all_ngrams = []
    for i in range(1, n + 1):
        ngrams = count_ngrams(text, i)
        ngrams = categorical_log_probabilities(ngrams)

        for j, sub_ngrams in enumerate(all_ngrams):
            for ng in ngrams:
                ngrams[ng] -= sub_ngrams[ng[-(j + 1):]]

        all_ngrams.append(ngrams)

    all_ngrams.reverse()
    return all_ngrams

def categorical_log_probabilities(counts):
    # In log-space, we can treat multiplication as addition and division
    # as subtraction. That makes calculations on probabilities much less
    # prone to numerical error.

    # This is equivalent to dividing the given ngram count by the total
    # number of ngrams.
    log_total = math.log(sum(counts.values()))
    return {key: math.log(counts[key]) - log_total for key in counts}

def count_ngrams(seq, n=3):
    # Count the ngrams generated by moving an n-character window over the text.
    ngrams = (seq[i:i + n] for i in range(len(seq) - n + 1))
    return dict(collections.Counter(ngrams))

def token_probability(text, ngram_probabilities):
    text = uniform_whitespace(text)
    text = text.split()

    ngram_max = len(ngram_probabilities)
    pvals = []
    tokens = []
    for raw_token in text:
        # Add whitespace to mark beginning of token.
        token = ' {}'.format(raw_token.strip().lower())
        ngram_p = 0
        for end in range(2, len(token) + 1):
            start = max(0, end - ngram_max)
            ngram_p += lookup_ngram(token[start:end], ngram_probabilities)
        pvals.append(ngram_p)

        # Remove initial space (but leave final space).
        tokens.append(raw_token)

    return [(t, p) for t, p in zip(tokens, pvals) if t.strip()]

def stream_probability(text, ngram_probabilities):
    text = uniform_whitespace(text)

    ngram_max = len(ngram_probabilities)
    tokens = []
    token = []
    pvals = []
    token_p = 0
    for end in range(1, len(text)):
        # Find the ngram. Add its log probability to the current token's
        # log probability, and append the last character to the current token.
        start = max(0, end - ngram_max)
        ngram = text[start:end].lower()
        token_p += lookup_ngram(ngram, ngram_probabilities)
        last_c = text[end - 1]
        token.append(last_c)

        # If the current token is whitespace, start a new token.
        # The `uniform_whitespace` call above should guarantee that all
        # strings of whitespace have been collapsed to a single space.
        if last_c == ' ':
            pvals.append(token_p)
            token_p = 0
            tokens.append(''.join(token))
            token = []

    return [(t, p) for t, p in zip(tokens, pvals) if t.strip()]

def lookup_ngram(init_ngram, ngram_probabilities):
    if not init_ngram:
        return 0

    ngram_max = len(init_ngram)

    start = 0
    for ngram_table in ngram_probabilities[-ngram_max:]:
        ngram = init_ngram[start:]
        if ngram in ngram_table:
            return ngram_max * ngram_table[ngram] / len(ngram)
        else:
            start += 1

    # If we get to this point, then the unigram itself is unkown. We
    # arbitrarily assign it a moderate log probability of -2 * ngram_max
    return -2.0 * ngram_max

class GibbsSampler(object):
    def __init__(self, ngram_probabilities,
                 n_cycles, burn_in, display_interval,
                 jump_probability):

        self.ngram_probabilities = ngram_probabilities
        self.ngram_size = len(ngram_probabilities)

        self.n_cycles = n_cycles
        self.burn_in = burn_in
        self.display_interval = display_interval
        self.jump_probability = jump_probability

        self.coded_string = None

    def gibbs_cycle(self, coded_string):
        self.coded_string = coded_string

        init_ngram_p = self.ngram_probabilities[self.ngram_size - 1]
        key = init_key(init_ngram_p, coded_string)

        max_key = key[:]
        max_prob = self.gibbs_probability(key)

        # The main loop. We cycle many times over two steps...
        for i in range(self.n_cycles):
            # First, we "shuffle" a pair of cipher characters, but in a way
            # that's weighted toward better results.
            offset = random.randrange(0, len(key))
            for key_ix in range(len(key)):
                self.gibbs_sample(key_ix, offset, key)

            # Second, we check to see if we've made a breakthrough. If so,
            # we see how much more ground we can claim quickly and greedily.
            prob = self.gibbs_probability(key)
            if prob >= max_prob * 1.5:
                new_prob, new_key = self.maximize_key(key)
                if new_prob > max_prob:
                    max_prob, max_key = new_prob, new_key

            if i % self.display_interval == 0:
                print("Iteration {}:".format(i))
                self.display_key("Current key", key)
                self.display_key("Most probable key", max_key)

        print('******************')
        print('** Final result **')
        print('******************')
        print()
        self.display_key("Most probable key", max_key)

    def display_key(self, label, key):
        coded_string = self.coded_string

        prob = self.gibbs_probability(key)
        prob_per_char = prob / len(coded_string)

        print('{}: '.format(label))
        print()
        print([key[x] for x in range(len(key))])
        print()
        print('{} output: '.format(label))
        print()
        print(''.join(key[s] for s in coded_string))
        print()
        print('{} log probability, total and per character: '.format(label))
        print()
        print('{}, {}'.format(prob, prob_per_char))
        print()

    def key_from_samples(self, samples, key):
        # This picks a key given a colleciton of samples. But hill-climbing
        # works better, so this code isn't used anymore.
        top = samples.most_common()
        sample_key = {}
        symbols = set(range(len(key)))
        chars = set(key)

        for (symbol, char), count in top:
            if symbol in symbols and char in chars:
                sample_key[symbol] = char
                symbols.remove(symbol)
                chars.remove(char)

        for symbol in range(len(key)):
            if symbol not in sample_key:
                sample_key[symbol] = 'X'

        return [sample_key[x] for x in range(len(sample_key))]

    def maximize_key(self, key, max_prob=None):
        prob = self.gibbs_probability(key)
        if max_prob is None:
            max_prob = prob - 1

        key = key[:]
        i = 0
        while prob > max_prob:
            i += 1
            max_prob = prob
            offset = random.randrange(0, len(key))
            for key_ix in range(len(key)):
                self.hill_step(key_ix, offset, key)
            prob = self.gibbs_probability(key)

        return max_prob, key

    def gibbs_sample(self, start, offset, key):
        # For every index in key following `start`, swap index with current
        # index and recalculate probability. Then pick an index pair to swap,
        # based on the calculated probabilities. Occasionally, this totally
        # ignores reason and picks a completely random pair to swap.
        # (This behavior is goverend by `self.jump_probability`).

        # Accepts an offset parameter to cut the "deck" before shuffling; this
        # seems to reduce the amount of time spent in bad local optima.

        start_ix = (start + offset) % len(key)
        if random.random() < self.jump_probability:
            swap_ix = random.randrange(start, len(key))
            swap_ix = (swap_ix + offset) % len(key)
        else:
            probs = []
            new_key = []
            for ix in range(start, len(key)):
                ix = (ix + offset) % len(key)
                new_key[:] = key
                new_key[ix], new_key[start_ix] = new_key[start_ix], new_key[ix]
                p = self.gibbs_probability(new_key)
                probs.append(p)

            swap_ix = categorical_sample(probs)
            swap_ix = (swap_ix + start_ix) % len(key)

        key[start_ix], key[swap_ix] = key[swap_ix], key[start_ix]

    def hill_step(self, start, offset, key):
        # Same as above, but no randomization; this always takes the maximum.
        # Accepts an offset parameter to cut the "deck" before shuffling.

        start_ix = (start + offset) % len(key)
        probs = []
        new_key = []
        for ix in range(start, len(key)):
            ix = (ix + offset) % len(key)
            new_key[:] = key
            new_key[ix], new_key[start_ix] = new_key[start_ix], new_key[ix]
            p = self.gibbs_probability(new_key)
            probs.append(p)

        swap_ix = max(range(len(probs)), key=probs.__getitem__)
        swap_ix = (swap_ix + start_ix) % len(key)

        key[start_ix], key[swap_ix] = key[swap_ix], key[start_ix]

    # A slightly more correct (but possibly slower)
    # calculation than the ones below.
    def gibbs_probability(self, key):
        ngram_p = self.ngram_probabilities
        ngram_max = self.ngram_size
        decoded_string = ''.join([key[c] for c in self.coded_string])

        p = 0
        string_end = len(decoded_string)
        for end in range(1, string_end + 1):
            start = max(0, end - ngram_max)
            p += lookup_ngram(decoded_string[start:end], ngram_p)
        return p

    def gibbs_probability_5grams(self, key):
        # A fast 5-gram benchmark for speeding up the gibbs probabilitiy
        # code below. The generic gibbs (below) is currently about 90%
        # as fast as this on pypy -- much slower using vanilla python
        # or cython though.
        quintgram_p, quadgram_p, trigram_p, bigram_p, unigram_p = \
            self.ngram_probabilities

        decoded_string = ''.join(key[c] for c in self.coded_string)
        decoded_ngrams = [decoded_string[i:i + 5] for i in
                          range(0, len(decoded_string) - 4)]
        p = 0

        # Next character probabilites. Here, recall that log probabilities
        # are negative so additions and multiplications make values
        # smaller. The multipliers here ensure that when we have only
        # low-n-gram information available, we don't over-weight that
        # information. More intelligent approaches might substitute
        # probabilities based on character class-grams; for example,
        # we might reasonably guess that a character following three
        # vowels is highly unlikely to be a vowel itself.
        for decoded_ng in decoded_ngrams:
            if decoded_ng in quintgram_p:
                p += quintgram_p[decoded_ng]
            elif decoded_ng[1:] in quadgram_p:
                p += quadgram_p[decoded_ng[1:]] * 5.0 / 4
            elif decoded_ng[2:] in trigram_p:
                p += trigram_p[decoded_ng[2:]] * 5.0 / 3
            elif decoded_ng[3:] in bigram_p:
                p += bigram_p[decoded_ng[3:]] * 5.0 / 2
            else:
                p += unigram_p[decoded_ng[4:]] * 5.0
        return p

    def gibbs_probability_original(self, key):
        ngram_p = self.ngram_probabilities
        ngram_max = self.ngram_size
        decoded_string = ''.join([key[c] for c in self.coded_string])

        p = 0
        string_end = len(decoded_string)
        for end in range(ngram_max, string_end):
            start = end - ngram_max
            for ngp in ngram_p:
                ngram = decoded_string[start:end]
                if ngram in ngp:
                    ngram_size = end - start
                    p += ngram_max * ngp[ngram] / (ngram_size)
                    break
                start += 1
        return p

def categorical_sample(probs):
    maxp = max(probs)
    probs = [math.exp(p - maxp) for p in probs]
    total = sum(probs)
    if total == 0:
        probs = [1] * len(probs)
        total = sum(probs)
    r = random.random()
    acc = 0
    for i, p in enumerate(probs):
        acc += float(p) / total
        if r <= acc:
            return i

def init_key(model_ngrams, encoded_mystery):
    n = len(next(iter(model_ngrams.keys())))
    mystery_ngrams = count_ngrams(tuple(encoded_mystery), n)

    ordered_model = sorted(model_ngrams,
                           key=model_ngrams.__getitem__,
                           reverse=True)
    ordered_mystery = sorted(mystery_ngrams,
                             key=mystery_ngrams.__getitem__,
                             reverse=True)

    alphabet = set(c for ng in model_ngrams for c in ng)
    indices = set(range(len(alphabet)))

    key = [None] * len(alphabet)
    assigned = set()
    for model_ng, mystery_ng in zip(ordered_model, ordered_mystery):
        for ix, char in zip(mystery_ng, model_ng):
            if ix in assigned or char in assigned:
                continue
            key[ix] = char
            assigned.add(ix)
            assigned.add(char)

    open_indices = indices - assigned
    unassigned_chars = alphabet - assigned
    for ix, char in zip(open_indices, unassigned_chars):
        key[ix] = char

    return key

def decode(args):
    if args.model_text:
        model_texts = [t for fn in args.model_text for t in load_text(fn)]
        model_text = ' '.join(model_texts)
        ngram_probabilities = get_ngram_probabilities(model_text,
                                                      args.ngram_size)
        chars = list(set(model_text))
    else:
        ngram_probabilities = load_saved_model(args.model_compiled)
        chars = list(set(ngram_probabilities[-1]))

    random.shuffle(chars)
    code = {c: i for i, c in enumerate(chars)}

    mystery = load_text(args.mystery)[0]

    encoded_mystery = [code[c] for c in mystery]

    print("Code:")
    print([c for i, c in sorted((i, c) for c, i in code.items())])

    # Decide on a sample length based on the number of possible
    # characters...  should be something like 2x or 3x smallest
    # sample, then that value ** 2, ** 3, and so on. ?
    # mystery_len = len(mystery)
    # mystery_loglen = math.log(mystery_len, 10)
    # mystery_chunks = [

    decoder = GibbsSampler(ngram_probabilities,
                           args.num_cycles, args.burn_in,
                           args.display_interval,
                           args.jump_probability)
    decoder.gibbs_cycle(encoded_mystery)

def token_fit(probability, token, pad=0):
    return (probability / (len(token) + pad), token)

def clean(args):
    if args.model_text:
        model_text = [t for fn in args.model_text for t in load_text(fn)]
        model_text = ' '.join(model_text).lower()
        model_text = uniform_whitespace(model_text)
        ngram_probabilities = get_ngram_probabilities(model_text,
                                                      args.ngram_size)
    else:
        ngram_probabilities = load_saved_model(args.model_compiled)

    input_texts = (
        text for filename in args.input
        for text in load_text(filename)
    )
    token_ps = (
        token_probability_clean(text, ngram_probabilities, args)
        for text in input_texts
    )
    p_tokens = ([token_fit(p, t) for (t, p) in text
                 if len(t) > args.min_characters]
                for text in token_ps)
    p_tokens_flat = (token_fit(p, t)
                     for text in token_ps
                     for (t, p) in text
                     if len(t) > args.min_characters)

    if args.output_format == 'allterms':
        clean_display_all(args, p_tokens_flat, -args.threshold)
    elif args.output_format == 'negterms':
        clean_display_neg(args, p_tokens_flat, -args.threshold)
    elif args.output_format == 'tagged':
        clean_display_tagged(args, p_tokens, -args.threshold)
    elif args.output_format == 'stripped':
        clean_display_stripped(args, p_tokens, -args.threshold)
    elif args.output_format == 'totals':
        clean_display_totals(args, p_tokens_flat, -args.threshold)
    elif args.output_format == 'save':
        clean_save_stripped(args, p_tokens, -args.threshold)

def token_probability_clean(text, ngram_probabilities, args=None):
    if args is not None:
        if args.remove_punctuation:
            text = strip_punctuation(text)
        if args.remove_digits:
            text = strip_digits(text)

    return token_probability(text, ngram_probabilities)

def stream_probability_clean(text, ngram_probabilities, args=None):
    if args is not None:
        if args.remove_punctuation:
            text = strip_punctuation(text)
        if args.remove_digits:
            text = strip_digits(text)

    return stream_probability(text, ngram_probabilities)

def clean_display_all(args, p_token, threshold):
    print('All tokens, with log probabilities.')
    for p, token in p_token:
        if p > threshold:
            print('{:20.20} {:8.5f}'.format(token, p))
        else:
            print('{:20.20} {:8.5f}      ***'.format(token, p))
    print()

def clean_display_neg(args, p_token, threshold):
    negterms = sorted((p, t) for p, t in p_token if p <= threshold)
    negterms.reverse()
    for p, token in negterms:
        print('{:20.20} {:8.5f}'.format(token, p))
    print()

def clean_display_tagged(args, p_orig, threshold):
    for i, p_o in enumerate(p_orig):
        tokens = ['{} '.format(o.strip()) if p > threshold else
                  '<<{}:{:6.3}>> '.format(o.strip(), p)
                  for p, o in p_o]
        text = ''.join(tokens)
        print('Document {}:'.format(i))
        print()
        for line in textwrap.wrap(text):
            print(line)
        print()

def clean_display_stripped(args, p_orig, threshold):
    for i, p_o in enumerate(p_orig):
        tokens = [o for p, o in p_o if p > threshold]
        text = ' '.join(tokens)
        for line in textwrap.wrap(text):
            print(line)
        print()
        print('Document {}:'.format(i))
        print()

def clean_save_stripped(args, p_orig, threshold):
    for i, (filename, p_o) in enumerate(zip(args.input, p_orig)):
        tokens = [o for p, o in p_o if p > threshold]
        text = ' '.join(tokens)
        filename, ext = os.path.splitext(filename)
        filename = '{}-stripped{}'.format(filename, ext)
        with open(filename, 'w', encoding='utf-8') as out:
            for line in textwrap.wrap(text):
                out.write(line)
                out.write('\n')

def clean_display_totals(args, p_token, threshold):
    pos_count = {}
    neg_count = {}
    tok_list = collections.defaultdict(list)
    for p, t in p_token:
        if p > threshold:
            pos_count[t] = pos_count.get(t, 0) + 1
        else:
            neg_count[t] = neg_count.get(t, 0) + 1
        tok_list[t].append(p)

    tok_avg = {t: sum(p) / len(p) for t, p in tok_list.items()}

    pos_set = set(pos_count)
    neg_set = set(neg_count)
    both_set = pos_set & neg_set
    all_set = pos_set | neg_set

    print('{} total tokens'.format(sum(pos_count.values()) +
                                   sum(neg_count.values())))
    print('{} positive tokens'.format(sum(pos_count.values())))
    print('{} negative tokens'.format(sum(neg_count.values())))
    print()

    print('{} total types'.format(len(all_set)))
    print('{} positive types'.format(len(pos_set)))
    print('{} negative types'.format(len(neg_set)))
    print('{} overlapping types'.format(len(both_set)))
    print()

    neg_sorted = sorted(neg_set, key=tok_avg.get, reverse=True)
    neg_output = '\n'.join('{:40} {}'.format(t, tok_avg[t])
                           for t in neg_sorted[0:200])
    print('Most probable negative types:')
    print(neg_output)
    print()

def save_model(args):
    model_text = [text for fn in args.model_file for text in load_text(fn)]
    model_text = ' '.join(model_text)

    if args.lower:
        model_text = model_text.lower()

    if args.uniform_whitespace:
        model_text = uniform_whitespace(model_text)

    ngram_probabilities = get_ngram_probabilities(model_text, args.ngram_size)
    with gzip.open(args.output, 'wt') as out:
        json.dump(ngram_probabilities, out)

def parse_args():
    parser = argparse.ArgumentParser(
        description='A set of tools based on a '
        'Naive Bayes charater model that does a decent job of distinguishing '
        'non-words from words given an input text and one or more model texts. '
        'These tools require no preprocessing or tokenization, and so can work '
        'with very messy data or unknown character sets.'
    )

    commands = parser.add_subparsers(
        title='Available Commands',
        description='For more help, the -h/--help option for each command.'
    )

    gsd_parser = commands.add_parser(
        'decode', conflict_handler='resolve',
        help='Use a modified version of Gibbs sampling to solve substitution '
        'ciphers. (The "Gibbs sampler" here may actually be closer to a '
        'metropolis-hastings sampler, because of the way it uses heuristics '
        'to escape local maxima.'
    )
    gsd_parser.add_argument(
        '-c', '--num-cycles', type=int, default=100,
        metavar='number', help='The total number of gibbs sampling cycles to '
        'complete.'
    )
    gsd_parser.add_argument(
        '-b', '--burn-in', type=int, default=20,
        metavar='number', help='The number of samples to discard before '
        'accumulating samples. Defaults to 20.'
    )
    gsd_parser.add_argument(
        '-d', '--display-interval', type=int, default=1,
        metavar='number', help='The frequency with which to display current '
        'key information.'
    )
    gsd_parser.add_argument(
        '-j', '--jump-probability', type=float,
        default=0.025, metavar='0.0-1.0', help='The probability of jumping '
        'from one state to another randomly. This is a heuristic for '
        'escaping local minima.'
    )
    gsd_parser.add_argument(
        '-n', '--ngram-size', type=int, default=5,
        metavar='number', choices=[2, 3, 4, 5, 6],
        help='The number of characters to use for the ngram model. Defaults '
        'to 5. Models larger than 6 tend to underperform.'
    )

    gsd_model_mutex = gsd_parser.add_mutually_exclusive_group(required=True)
    gsd_model_mutex.add_argument(
        '-m', '--model-text', type=str,
        metavar='filename', action='append', help='The name of a file with '
        'which to analyize character n-gram frequencies. May be used '
        'multiple times, and must be used at least once unless '
        '`--model-compiled` is used instead.'
    )
    gsd_model_mutex.add_argument(
        '-M', '--model-compiled', type=str,
        metavar='filename', help='The name of a model file saved previously '
        'using the `save` command. May not be used at the same time as '
        '`--model-text`, and may only be used once.'
    )

    gsd_parser.add_argument(
        'mystery', type=str, metavar='filename',
        help='The name of a file containing the text to be encrypted '
        'and decrypted.'
    )
    gsd_parser.set_defaults(command=decode)

    clean_parser = commands.add_parser(
        'clean', conflict_handler='resolve',
        help='Use a Naive Bayes character sequence model to identify unlikely '
        'words.'
    )
    clean_parser.add_argument(
        '-n', '--ngram-size', type=int, default=5,
        metavar='number', choices=[2, 3, 4, 5, 6],
        help='The number of characters to use for the ngram model. Defaults '
        'to 5. Models larger than 6 tend to underperform.'
    )
    clean_parser.add_argument(
        '-c', '--min-characters', type=int, default=1,
        metavar='number', help='The minimum number of characters a word must '
        'have to pass. Words with fewer characters are tagged and stripped '
        'even if they are judged to be likely words. Defaults to 1.'
    )
    clean_parser.add_argument(
        '-p', '--remove-punctuation', action='store_true', default=False,
        help='Replace punctuation (as defined by `string.punctuation`) with '
        'whitespace prior to processing.'
    )
    clean_parser.add_argument(
        '-d', '--remove-digits', action='store_true', default=False,
        help='Replace digits with whitespace prior to processing.'
    )

    clean_model_mutex = clean_parser.add_mutually_exclusive_group(required=True)
    clean_model_mutex.add_argument(
        '-m', '--model-text', type=str,
        metavar='filename', action='append', help='The name of a file with '
        'which to analyize character n-gram frequencies. May be used '
        'multiple times, and must be used at least once unless '
        '`--model-compiled` is used instead.'
    )
    clean_model_mutex.add_argument(
        '-M', '--model-compiled', type=str,
        metavar='filename', help='The name of a model file saved previously '
        'using the `save` command. May not be used at the same time as '
        '`--model-text`, and may only be used once.'
    )

    clean_parser.add_argument(
        '-o', '--output-format', type=str, default='tagged',
        choices=['allterms', 'negterms', 'totals',
                 'tagged', 'stripped', 'save'],
        help='The output to generate. `allterms` displays all terms with '
        'negative terms marked. `negterms` displays only negative (non-word) '
        'terms, sorted from most to least likely. `tagged` displays the input '
        'text in its original order, but with negative terms tagged. '
        '`stripped` displays the input text with negative terms removed.'
    )
    clean_parser.add_argument(
        '-t', '--threshold', type=float, default=4,
        help='The likelihood threshold to be used for evaluation.'
    )
    clean_parser.add_argument(
        'input', type=str, metavar='filename',
        nargs='+',
        help='The name of a file or files containing the text to be processed.'
    )
    clean_parser.set_defaults(command=clean)

    save_parser = commands.add_parser(
        'save', conflict_handler='resolve',
        help='Save an efficient copy of a Naive Bayes character '
        'sequence model.'
    )
    save_parser.add_argument(
        '-m', '--model-file', type=str,
        metavar='filename', action='append', help='The name of a file with '
        'which to analyize character n-gram frequencies. May be specified '
        'multiple times, and must be specified at least once.'
    )
    save_parser.add_argument(
        '-l', '--lower', action='store_true', default=False,
        help='Convert all characters to lowercase before saving. Models that '
        'use only lower-case characters have better performance given the '
        'same number of characters, but they cannot handle capitalization '
        'gracefully, and so can only be used on input texts that have '
        'also been converted.'
    )
    save_parser.add_argument(
        '-w', '--uniform-whitespace', action='store_true', default=False,
        help='Make all whitespace uniform before saving. All whitespace '
        'characters or strings of whitespace-only characters will be '
        'replaced with a single space.'
    )
    save_parser.add_argument(
        '-n', '--ngram-size', type=int, default=5,
        metavar='number', choices=[2, 3, 4, 5, 6],
        help='The number of characters to use for the ngram model. Defaults '
        'to 5. Models larger than 6 tend to underperform.'
    )
    save_parser.add_argument(
        'output', type=str, metavar='filename',
        help='The name of the model save file.'
    )
    save_parser.set_defaults(command=save_model)

    return parser.parse_args()

def main(argv):
    args = parse_args()
    args.command(args)

if __name__ == '__main__':
    main(sys.argv)
