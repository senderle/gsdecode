#!/usr/local/bin/pypy

import math
import collections
import random
import sys
import re
import string
import argparse
import textwrap
import os

def load_text(fn, space_pad=3):
    with open(fn) as datafile:
        text_raw = datafile.read()
        rex = '[^{}]'.format(string.printable)
        text = re.sub(rex, ' ', text_raw)
        return text

_space_rex = re.compile('\s+')
def uniform_whitespace(text):
    return _space_rex.sub(' ', text)

def get_ngram_probabilities(text, n):
    if n < 1:
        raise ValueError("Minimum ngram size is 1")

    all_ngrams = []
    for i in range(1, n + 1):
        ngrams = count_ngrams(text, i)
        ngrams = categorical_log_probabilities(ngrams)

        # Here we calculate the probability of the given ngram as the
        # probabilitiy of the last character appearing given the previous
        # characters. This way, we can substitute lower n-gram stats for
        # missing higher n-gram stats in an internally consistent way.
        # This amounts to the assumption that the missing higher n-grams
        # won't affect the marginal probability for lower n-grams.
        for j, sub_ngrams in enumerate(all_ngrams):
            for ng in ngrams:
                ngrams[ng] -= sub_ngrams[ng[:j + 1]]

        all_ngrams.append(ngrams)

    all_ngrams.reverse()
    return all_ngrams

# This is literally identical to the above function, with one exception:
# Instead of factoring out the leading lower n-grams, it factors out the
# trailing lower n-grams. In other words, where the above tries to guess
# the probability of the last character given the first few, this tries
# to guess the probability of the first character given the last few.
def get_ngram_head_probabilities(text, n):
    if n < 1:
        raise ValueError("Minimum ngram size is 1")

    all_ngrams = []
    for i in range(1, n + 1):
        ngrams = count_ngrams(text, i)
        ngrams = categorical_log_probabilities(ngrams)

        for j, sub_ngrams in enumerate(all_ngrams):
            for ng in ngrams:
                ngrams[ng] -= sub_ngrams[ng[-(j + 1):]]

        all_ngrams.append(ngrams)

    all_ngrams.reverse()
    return all_ngrams

def categorical_log_probabilities(counts):
    # In log-space, we can treat multiplication as addition and division
    # as subtraction. That makes calculations on probabilities much less
    # prone to numerical error.

    # This is equivalent to dividing the given ngram count by the total
    # number of ngrams.
    log_total = math.log(sum(counts.values()))
    return {key: math.log(counts[key]) - log_total for key in counts}

def count_ngrams(seq, n=3):
    # Count the ngrams generated by moving an n-character window over the text.
    ngrams = (seq[i:i + n] for i in range(len(seq) - n + 1))
    return dict(collections.Counter(ngrams))

def gibbs_probability(text, ngram_probabilities):
    ngram_max = len(ngram_probabilities)

    p = 0
    string_end = len(text)
    for end in range(ngram_max, string_end):
        start = end - ngram_max
        for ngp in ngram_probabilities:
            ngram = text[start:end]
            if ngram in ngp:
                ngram_size = end - start
                p += ngram_max * ngp[ngram] / ngram_size
                break
            start += 1
    return p

def token_probability(text, ngram_probabilities):
    ngram_max = len(ngram_probabilities)
    text = uniform_whitespace(text)

    tokens = []
    token = []
    pvals = []
    p = 0
    string_end = len(text)
    for end in range(string_end):
        start = end - ngram_max
        for ngp in ngram_probabilities:
            ngram = text[start:end]
            if ngram in ngp:
                ngram_size = end - start
                p += ngram_max * ngp[ngram] / ngram_size
                token.append(ngram[-1])
                if ngram[-1] == ' ':
                    pvals.append(p)
                    p = 0
                    tokens.append(''.join(token))
                    token = []
                break
            start += 1
    return [(t, p) for t, p in zip(tokens, pvals) if t.strip()]

class GibbsSampler(object):
    def __init__(self, ngram_probabilities,
                 n_cycles, burn_in, display_interval,
                 jump_probability):

        self.ngram_probabilities = ngram_probabilities
        self.ngram_size = len(ngram_probabilities)

        self.n_cycles = n_cycles
        self.burn_in = burn_in
        self.display_interval = display_interval
        self.jump_probability = jump_probability

        self.coded_string = None

    def gibbs_cycle(self, coded_string):
        self.coded_string = coded_string

        init_ngram_p = self.ngram_probabilities[self.ngram_size - 1]
        key = init_key(init_ngram_p, coded_string)

        max_key = key[:]
        max_prob = self.gibbs_probability(key)

        # The main loop. We cycle many times over two steps...
        for i in range(self.n_cycles):
            # First, we "shuffle" a pair of cipher characters, but in a way
            # that's weighted toward better results.
            offset = random.randrange(0, len(key))
            for key_ix in range(len(key)):
                self.gibbs_sample(key_ix, offset, key)

            # Second, we check to see if we've made a breakthrough. If so,
            # we see how much more ground we can claim quickly and greedily.
            prob = self.gibbs_probability(key)
            if prob >= max_prob * 1.5:
                new_prob, new_key = self.maximize_key(key)
                if new_prob > max_prob:
                    max_prob, max_key = new_prob, new_key

            if i % self.display_interval == 0:
                print("Iteration {}:".format(i))
                self.display_key("Current key", key)
                self.display_key("Most probable key", max_key)

        print('******************')
        print('** Final result **')
        print('******************')
        print()
        self.display_key("Most probable key", max_key)

    def display_key(self, label, key):
        coded_string = self.coded_string

        prob = self.gibbs_probability(key)
        prob_per_char = prob / len(coded_string)

        print('{}: '.format(label))
        print()
        print([key[x] for x in range(len(key))])
        print()
        print('{} output: '.format(label))
        print()
        print(''.join(key[s] for s in coded_string))
        print()
        print('{} log probability, total and per character: '.format(label))
        print()
        print('{}, {}'.format(prob, prob_per_char))
        print()
        # self.display_tokens(key)

    def display_tokens(self, key):
        text = ''.join(key[s] for s in self.coded_string)
        token_p = token_probability(text, self.ngram_probabilities)
        for token, p in token_p:
            p = p / (len(token) + 2)
            if p > -3.25:
                print('{} {}'.format(token, p))
        print()

    def key_from_samples(self, samples, key):
        # This picks a key given a colleciton of samples. But hill-climbing
        # works better, so this code isn't used anymore.
        top = samples.most_common()
        sample_key = {}
        symbols = set(range(len(key)))
        chars = set(key)

        for (symbol, char), count in top:
            if symbol in symbols and char in chars:
                sample_key[symbol] = char
                symbols.remove(symbol)
                chars.remove(char)

        for symbol in range(len(key)):
            if symbol not in sample_key:
                sample_key[symbol] = 'X'

        return [sample_key[x] for x in range(len(sample_key))]

    def maximize_key(self, key, max_prob=None):
        print('Checking for better solutions using hill climbing...')
        print()

        prob = self.gibbs_probability(key)
        if max_prob is None:
            max_prob = prob - 1

        key = key[:]
        i = 0
        while prob > max_prob:
            i += 1
            max_prob = prob
            offset = random.randrange(0, len(key))
            for key_ix in range(len(key)):
                self.hill_step(key_ix, offset, key)
            prob = self.gibbs_probability(key)
            sys.stdout.write('\rHill climbing step {}'.format(i))
            sys.stdout.flush()

        self.display_key("Best hill climbing key", key)

        return max_prob, key

    def gibbs_sample(self, start, offset, key):
        # For every index in key following `start`, swap index with current
        # index and recalculate probability. Then pick an index pair to swap,
        # based on the calculated probabilities. Occasionally, this totally
        # ignores reason and picks a completely random pair to swap.
        # (This behavior is goverend by `self.jump_probability`).

        # Accepts an offset parameter to cut the "deck" before shuffling; this
        # seems to reduce the amount of time spent in bad local optima.

        start_ix = (start + offset) % len(key)
        if random.random() < self.jump_probability:
            swap_ix = random.randrange(start, len(key))
            swap_ix = (swap_ix + offset) % len(key)
        else:
            probs = []
            new_key = []
            for ix in range(start, len(key)):
                ix = (ix + offset) % len(key)
                new_key[:] = key
                new_key[ix], new_key[start_ix] = new_key[start_ix], new_key[ix]
                p = self.gibbs_probability(new_key)
                probs.append(p)

            swap_ix = categorical_sample(probs)
            swap_ix = (swap_ix + start_ix) % len(key)

        key[start_ix], key[swap_ix] = key[swap_ix], key[start_ix]

    def hill_step(self, start, offset, key):
        # Same as above, but no randomization; this always takes the maximum.
        # Accepts an offset parameter to cut the "deck" before shuffling.

        start_ix = (start + offset) % len(key)
        probs = []
        new_key = []
        for ix in range(start, len(key)):
            ix = (ix + offset) % len(key)
            new_key[:] = key
            new_key[ix], new_key[start_ix] = new_key[start_ix], new_key[ix]
            p = self.gibbs_probability(new_key)
            probs.append(p)

        swap_ix = max(range(len(probs)), key=probs.__getitem__)
        swap_ix = (swap_ix + start_ix) % len(key)

        key[start_ix], key[swap_ix] = key[swap_ix], key[start_ix]

    def gibbs_probability_5grams(self, key):
        # A fast 5-gram benchmark for speeding up the gibbs probabilitiy
        # code below. The generic gibbs (below) is currently about 90%
        # as fast as this on pypy -- much slower using vanilla python
        # or cython though.
        quintgram_p, quadgram_p, trigram_p, bigram_p, unigram_p = \
            self.ngram_probabilities

        decoded_string = ''.join(key[c] for c in self.coded_string)
        decoded_ngrams = [decoded_string[i:i + 5] for i in
                          range(0, len(decoded_string) - 4)]
        p = 0

        # Next character probabilites. Here, recall that log probabilities
        # are negative so additions and multiplications make values
        # smaller. The multipliers here ensure that when we have only
        # low-n-gram information available, we don't over-weight that
        # information. More intelligent approaches might substitute
        # probabilities based on character class-grams; for example,
        # we might reasonably guess that a character following three
        # vowels is highly unlikely to be a vowel itself.
        for decoded_ng in decoded_ngrams:
            if decoded_ng in quintgram_p:
                p += quintgram_p[decoded_ng]
            elif decoded_ng[1:] in quadgram_p:
                p += quadgram_p[decoded_ng[1:]] * 5.0 / 4
            elif decoded_ng[2:] in trigram_p:
                p += trigram_p[decoded_ng[2:]] * 5.0 / 3
            elif decoded_ng[3:] in bigram_p:
                p += bigram_p[decoded_ng[3:]] * 5.0 / 2
            else:
                p += unigram_p[decoded_ng[4:]] * 5.0
        return p

    def gibbs_probability(self, key):
        ngram_p = self.ngram_probabilities
        ngram_max = self.ngram_size
        decoded_string = ''.join([key[c] for c in self.coded_string])

        p = 0
        string_end = len(decoded_string)
        for end in range(ngram_max, string_end):
            start = end - ngram_max
            for ngp in ngram_p:
                ngram = decoded_string[start:end]
                if ngram in ngp:
                    ngram_size = end - start
                    p += ngram_max * ngp[ngram] / (ngram_size)
                    break
                start += 1
        return p

# Until I can make this a _pure cython_ function, there's no
# point in compiling this. It's faster to just run the above
# through pypy.

    # def gibbs_probability_ngrams(self, key):
    #     ngram_p = self.ngram_probabilities
    #     decoded_string = ''.join([key[c] for c in self.coded_string])
    #
    #     cdef int ngram_max, string_end, start, end
    #     cdef double p
    #     ngram_max = self.ngram_size

    #     p = 0
    #     string_end = len(decoded_string)
    #     for end in range(ngram_max, string_end):
    #         start = end - ngram_max
    #         for ngp in ngram_p:
    #             ngram = decoded_string[start:end]
    #             if ngram in ngp:
    #                 ngram_size = end - start
    #                 p += ngram_max * ngp[ngram] / (ngram_size)
    #                 break
    #             start += 1
    #     return p

def categorical_sample(probs):
    maxp = max(probs)
    probs = [math.exp(p - maxp) for p in probs]
    total = sum(probs)
    if total == 0:
        probs = [1] * len(probs)
        total = sum(probs)
    r = random.random()
    acc = 0
    for i, p in enumerate(probs):
        acc += float(p) / total
        if r <= acc:
            return i

def init_key(model_ngrams, encoded_mystery):
    n = len(next(iter(model_ngrams.keys())))
    mystery_ngrams = count_ngrams(tuple(encoded_mystery), n)

    ordered_model = sorted(model_ngrams,
                           key=model_ngrams.__getitem__,
                           reverse=True)
    ordered_mystery = sorted(mystery_ngrams,
                             key=mystery_ngrams.__getitem__,
                             reverse=True)

    alphabet = set(c for ng in model_ngrams for c in ng)
    indices = set(range(len(alphabet)))

    key = [None] * len(alphabet)
    assigned = set()
    for model_ng, mystery_ng in zip(ordered_model, ordered_mystery):
        for ix, char in zip(mystery_ng, model_ng):
            if ix in assigned or char in assigned:
                continue
            key[ix] = char
            assigned.add(ix)
            assigned.add(char)

    open_indices = indices - assigned
    unassigned_chars = alphabet - assigned
    for ix, char in zip(open_indices, unassigned_chars):
        key[ix] = char

    return key

def decode(args):
    model_texts = [load_text(fn) for fn in args.model_file]
    model_text = ' '.join(model_texts)
    ngram_probabilities = get_ngram_probabilities(model_text, args.ngram_size)

    chars = list(set(model_text))
    random.shuffle(chars)
    code = {c: i for i, c in enumerate(chars)}

    mystery = load_text(args.mystery)

    encoded_mystery = [code[c] for c in mystery]

    print("Code:")
    print([c for i, c in sorted((i, c) for c, i in code.items())])

    # Decide on a sample length based on the number of possible
    # characters...  should be something like 2x or 3x smallest
    # sample, then that value ** 2, ** 3, and so on. ?
    # mystery_len = len(mystery)
    # mystery_loglen = math.log(mystery_len, 10)
    # mystery_chunks = [

    decoder = GibbsSampler(ngram_probabilities,
                           args.num_cycles, args.burn_in,
                           args.display_interval,
                           args.jump_probability)
    decoder.gibbs_cycle(encoded_mystery)

def clean(args):
    model_text = [load_text(fn) for fn in args.model_file]
    model_text = ' '.join(model_text).lower()
    model_text = uniform_whitespace(model_text)
    ngram_probabilities = get_ngram_probabilities(model_text, args.ngram_size)

    raw_originals = [load_text(inf) for inf in args.input]
    originals = list(map(uniform_whitespace, raw_originals))
    input_text = ' '.join(originals).lower()
    token_p = token_probability(input_text, ngram_probabilities)

    p_token = [(p / (len(t) + 2), t) for t, p in token_p]
    p_orig = []
    start = 0
    for orig in originals:
        orig = orig.split()
        end = start + len(orig)
        p_token_chunk = p_token[start:end]
        p_o = [(p, o) for (p, t), o in zip(p_token_chunk, orig)]
        p_orig.append(p_o)
        start = end

    if args.output_format == 'allterms':
        clean_display_all(args, p_token, -args.threshold)
    elif args.output_format == 'negterms':
        clean_display_neg(args, p_token, -args.threshold)
    elif args.output_format == 'tagged':
        clean_display_tagged(args, p_orig, -args.threshold)
    elif args.output_format == 'stripped':
        clean_display_stripped(args, p_orig, -args.threshold)
    elif args.output_format == 'totals':
        clean_display_totals(args, p_token, -args.threshold)
    elif args.output_format == 'save-stripped':
        clean_save_stripped(args, p_orig, -args.threshold)

def clean_display_all(args, p_token, threshold):
    print('All tokens, with log probabilities.')
    for p, token in p_token:
        if p > threshold:
            print('{:20.20} {:8.5f}'.format(token, p))
        else:
            print('{:20.20} {:8.5f}      ***'.format(token, p))
    print()

def clean_display_neg(args, p_token, threshold):
    negterms = sorted((p, t) for p, t in p_token if p <= threshold)
    negterms.reverse()
    for p, token in negterms:
        print('{:20.20} {:8.5f}'.format(token, p))
    print()

def clean_display_tagged(args, p_orig, threshold):
    for i, p_o in enumerate(p_orig):
        tokens = ['{} '.format(o.strip()) if p > threshold else
                  '<<{}>> '.format(o.strip())
                  for p, o in p_o]
        text = ''.join(tokens)
        for line in textwrap.wrap(text):
            print(line)
        print()
        print('Document {}:'.format(i))
        print()

def clean_display_stripped(args, p_orig, threshold):
    for i, p_o in enumerate(p_orig):
        tokens = [o for p, o in p_o if p > threshold]
        text = ' '.join(tokens)
        for line in textwrap.wrap(text):
            print(line)
        print()
        print('Document {}:'.format(i))
        print()

def clean_save_stripped(args, p_orig, threshold):
    for i, filename, p_o in enumerate(zip(args.input, p_orig)):
        tokens = [o for p, o in p_o if p > threshold]
        text = ' '.join(tokens)
        filename, ext = os.path.splitext(filename)
        filename = '{}-stripped{}'.format(filename, ext)
        with open(filename, 'w', encoding='utf-8') as out:
            for line in textwrap.wrap(text):
                out.write(line)
                out.write('\n')

def clean_display_totals(args, p_token, threshold):
    pos = [t for p, t in p_token if p > threshold]
    neg = [t for p, t in p_token if p <= threshold]
    pos_set = set(pos)
    neg_set = set(neg)
    both_set = pos_set & neg_set
    all_set = pos_set | neg_set
    print('{} positive tokens'.format(len(pos)))
    print('{} negative tokens'.format(len(neg)))
    print('{} positive types'.format(len(pos_set)))
    print('{} negative types'.format(len(neg_set)))
    print('{} overlapping types'.format(len(both_set)))
    print('{} total types'.format(len(all_set)))

    print('Overlapping types:')
    print('\n'.join(textwrap.wrap(' '.join(both_set))))

def parse_args():
    parser = argparse.ArgumentParser(
        description='A set of tools based on a '
        'Naive Bayes charater model that does a decent job of distinguishing '
        'non-words from words given an input text and one or more model texts. '
        'These tools require no preprocessing or tokenization, and so can work '
        'with very messy data or unknown character sets.'
    )

    commands = parser.add_subparsers(
        title='Available Commands',
        description='For more help, the -h/--help option for each command.'
    )

    gsd_parser = commands.add_parser(
        'decode', conflict_handler='resolve',
        help='Use a modified version of Gibbs sampling to solve substitution '
        'ciphers. (The "Gibbs sampler" here may actually be closer to a '
        'metropolis-hastings sampler, because of the way it uses heuristics '
        'to escape local maxima.'
    )
    gsd_parser.add_argument(
        '-c', '--num-cycles', type=int, default=100,
        metavar='number', help='The total number of gibbs sampling cycles to '
        'complete.'
    )
    gsd_parser.add_argument(
        '-b', '--burn-in', type=int, default=20,
        metavar='number', help='The number of samples to discard before '
        'accumulating samples. Defaults to 20.'
    )
    gsd_parser.add_argument(
        '-d', '--display-interval', type=int, default=1,
        metavar='number', help='The frequency with which to display current '
        'key information.'
    )
    gsd_parser.add_argument(
        '-j', '--jump-probability', type=float,
        default=0.025, metavar='0.0-1.0', help='The probability of jumping '
        'from one state to another randomly. This is a heuristic for '
        'escaping local minima.'
    )
    gsd_parser.add_argument(
        '-n', '--ngram-size', type=int, default=5,
        metavar='number', choices=[2, 3, 4, 5, 6],
        help='The number of characters to use for the ngram model. Defaults '
        'to 5. Models larger than 6 tend to underperform.'
    )
    gsd_parser.add_argument(
        '-m', '--model-file', type=str,
        metavar='filename', action='append', help='The name of a file with '
        'which to analyize character n-gram frequencies. May be specified '
        'multiple times, and must be specified at least once.'
    )
    gsd_parser.add_argument(
        'mystery', type=str, metavar='filename',
        help='The name of a file containing the text to be encrypted '
        'and decrypted.'
    )
    gsd_parser.set_defaults(command=decode)

    clean_parser = commands.add_parser(
        'clean', conflict_handler='resolve',
        help='Use a Naive Bayes character sequence model to identify unlikely '
        'words.'
    )
    clean_parser.add_argument(
        '-n', '--ngram-size', type=int, default=5,
        metavar='number', choices=[2, 3, 4, 5, 6],
        help='The number of characters to use for the ngram model. Defaults '
        'to 5. Models larger than 6 tend to underperform.'
    )
    clean_parser.add_argument(
        '-m', '--model-file', type=str,
        metavar='filename', action='append', help='The name of a file with '
        'which to analyize character n-gram frequencies. May be specified '
        'multiple times, and must be specified at least once.'
    )
    clean_parser.add_argument(
        '-o', '--output-format', type=str, default='tagged',
        choices=['allterms', 'negterms', 'totals', 'tagged', 'stripped'],
        help='The output to generate. `allterms` displays all terms with '
        'negative terms marked. `negterms` displays only negative (non-word) '
        'terms, sorted from most to least likely. `tagged` displays the input '
        'text in its original order, but with negative terms tagged. '
        '`stripped` displays the input text with negative terms removed.'
    )
    clean_parser.add_argument(
        '-t', '--threshold', type=float, default=3.25,
        help='The likelihood threshold to be used for evaluation.'
    )
    clean_parser.add_argument(
        'input', type=str, metavar='filename',
        nargs='+',
        help='The name of a file or files containing the text to be processed.'
    )
    clean_parser.set_defaults(command=clean)

    save_parser = commands.add_parser(
        'save', conflict_handler='resolve',
        help='Save an efficient copy of a Naive Bayes character '
        'sequence model'
    )
    save_parser.add_argument(
        '-m', '--model-file', type=str,
        metavar='filename', action='append', help='The name of a file with '
        'which to analyize character n-gram frequencies. May be specified '
        'multiple times, and must be specified at least once.'
    )
    save_parser.add_argument(
        'output', type=str, metavar='filename',
        help='The name of the model save file.'
    )

    return parser.parse_args()

def main(argv):
    args = parse_args()
    args.command(args)

if __name__ == '__main__':
    main(sys.argv)
